---
title: "SeifLM Project - Week Two"
date: "24-12-2025"
time: "18:00"
timeToRead: "10 minute read"
excerpt: "Deepdive into the SeifLM project, the Seif Stack RAG code agent, using FastAPI, Supabase, LangChain, and Next.js. Pt 2 of the SeifLM project."
tags: ["SideProject", "SeifLM", "Embedding pipeline", "transformers", "BAAI/bge-m3", "Supabase", "GPU compute"]
---

[Last week](/blog/seif-lm-week-one) I introduced the SeifLM project, a coding agent/ RAG model that uses code reasoning and is specialised in my favourite tech stack (FastAPI, Supabase, and Next.js).

Last week we ran into a hurdle with generating the embeddings and persisting them to Supabase due to hardware limitations. This week I will show you how I overcame this hurdle and got the embeddings into Supabase.

Let's refer to the TODO list from last week:
- [x] Create a scraper that scrapes the official documentation for FastAPI, Supabase, LangChain, and Next.js.
- [x] Create a chunking pipeline that chunks the scraped documentation into smaller pieces.
- [x] Create an embedding pipeline that converts the chunks into embeddings.
- [x] Setup Supabase for storing the embeddings.
- [ ] Store the embeddings in Supabase.
- [ ] Create a FastAPI endpoint that takes a question and returns the relevant code snippet and explanation.
- [ ] Create a Next.js frontend that allows users to ask questions and get answers from the agent.
- [ ] Create a Dockerfile for hosting Ollama
_________________

## Embedding pipeline
Due to hardware limitations, I had to use a different approach to generate the embeddings. I tried using `pyspark` to distribute the workload across my Mac. I tried `ray` as well. But it's not about those technologies, It was about my laptop not having a powerful enough GPU and enough memory.

The task at hand was to research GPU compute providers that allow you to run your code on their GPUs. I found a few notable options:
- [Paperspace](https://www.paperspace.com/)
- [Lambda Labs](https://lambdalabs.com/)
- [Modal](https://modal.com/)

But the one I went with was [ThunderCompute](https://thundercompute.io/). They have a free tier that allows you to run your code on their GPUs. I just take my script, transfer it to an instance with their GPU and run it.

I am able to do that because:
1. I prepared my data and uploaded all the chunks to Supabase Storage.
2. I created a script that downloads the chunks from Supabase Storage, generates the embeddings, and uploads them back to Supabase in a vector database.

My choice would be Modal of course if I were to have some sort of pipeline where I constantly generate embeddings. But since this is a one-off script, this will do (and its free!)
The reason I don't want to make an automated pipeline is simply because I want to expirement more with model architecture, makeing the agent, the reasoning, reviewing their thinking steps, etc..

Very simply, this script is split into 3 parts:
- retrieving the pickle files and deserialising them into `langchain Document` objects
- instantiating the model with cuda
- embedding and then inserting into supabase vector database

```python
# Setting up the model and supabase:
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
STORAGE_BUCKET = os.getenv("SUPABASE_BUCKET", "chunks")

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

device = "cuda" if torch.cuda.is_available() else "cpu"
logging.info(f"Using {device} for model instantiation")

model = SentenceTransformer("BAAI/bge-m3", device=device)
```

```python
# the embedding function
def embed_documents(docs: List[Document], batch_size: int = 8) -> List[dict]:
    texts = [doc.page_content for doc in docs]
    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=False).tolist()
    output = []
    for doc, emb in zip(docs, embeddings):
        output.append({
            "embedding": emb,
            "metadata": doc.metadata
        })
    return output
```
----

### How to use ThunderCompute
Well, you can just sign up, get the free $20 credits, choose a GPU and create an instance from it.

![GPU Pricing](/assets/blogs/Thundercompute-gpu.png)

The next steps would be:
1. Connecting to the instance via SSH
2. SCPing your files (dependencies, env file, script, etc.) to the instance
3. Make sure python is installed and the requirements are installed

For people who struggle to use the terminal for SSH, thundercloud also offers a VScode extension. I just used the terminal for speed of use.
4. Run the script and wait for it to finish
5. double check the embeddings are in the vector database
6. Shut down the instance to avoid being charged

![Script Logs](/assets/blogs/thundercloud-script.png)

![Supabase DB](/assets/blogs/script-supabase-results.png)

As you can see the chunks have all been inserted into the vector database. The embeddings are stored in the `embedding` column and the metadata is stored in the `metadata` column.

The next week(s) I will be focusing on designing the agentic architecture, testing the queries with similarity scoring and mapping out what I want to do with these embeddings.

The TODO list for next week is:
- [ ] experiment with similarity score and rankings of embeddings
- [ ] finetune a small model with the Nvidia code reasoning dataset
- [ ] create an agentic architecture, try out memory and long-term memory

